{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://redditmetrics.com/tophttp://redditmetrics.com/top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"I used the documentation in addition to the following video for help on this webscraping/API process:\n",
    "    https://www.youtube.com/watch?v=yexxcrPC7U8 \"\"\"\n",
    "\n",
    "import requests\n",
    "import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import praw\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import selenium\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.select import Select\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "driver = webdriver.PhantomJS(executable_path='/Users/thomas/Downloads/phantomjs-2.1.1-macosx/bin/phantomjs')\n",
    "driver.set_window_size(300, 200) \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below section is for reddit scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_replies(comment, reply_list):\n",
    "    \"\"\" recursive function that goes through the reply tree of a comment \"\"\"\n",
    "    reply_list.append({\"id\": comment.id, \"comment\": comment.body})\n",
    "    if len(comment._replies) > 0:\n",
    "        for r in comment._replies:\n",
    "            get_replies(r, reply_list)\n",
    "    return reply_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## This Cell is Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "117\n",
      "\n",
      "Starting Collection from /r/apple\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('Top Subreddits.csv')\n",
    "subreddit_list=[x for x in df['Reddit']]\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "Number_of_Posts=100\n",
    "Number_of_Comments=4\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "for i in range(117,155):\n",
    "    print \n",
    "    print i\n",
    "    print \n",
    "    count=0\n",
    "    current_time=time.time()\n",
    "    print \"Starting Collection from \"+subreddit_list[i]\n",
    "    subreddit_posts={}\n",
    "    json_name='/Users/thomas/GA-DSI/projects/projects-capstone/Work 2, Personalities/Subreddit Data/'\\\n",
    "                        +subreddit_list[i][3:]+'.json'\n",
    "    info_for_request='get top '+str(Number_of_Posts)+' posts w/ top '+str(Number_of_Comments)+\\\n",
    "                        ' comments and children comments from '+subreddit_list[i]+\\\n",
    "                        \"for Capstone project at General Assembly's DSI program by User: dingdong1111\"\n",
    "    r = praw.Reddit(user_agent=info_for_request)\n",
    "    r.login('dingdong1111', '01001759',disable_warning=True)\n",
    "    posts = r.get_subreddit(subreddit_list[i][3:]).get_top_from_year(limit=Number_of_Posts)\n",
    "    for post in posts:\n",
    "        count+=1\n",
    "        post_comments=[]\n",
    "        head_comments=[]\n",
    "        \n",
    "        post_id=post.id\n",
    "        post_title=post.title\n",
    "        post_score=post.score\n",
    "        post_url=post.url\n",
    "        post_date=datetime.datetime.fromtimestamp(post.created)\n",
    "#         post_author=post.author\n",
    "        \n",
    "        submission = r.get_submission(submission_id=post_id)\n",
    "        submission.replace_more_comments(limit=Number_of_Comments, threshold=0)\n",
    "        for comment in submission.comments:\n",
    "            \"\"\" loops through all comments to submission \"\"\"\n",
    "            x = []  # array that holds the comment and all replies\n",
    "            # get all replies and append it to our array for writing to file\n",
    "            post_comments.append(get_replies(comment, x))\n",
    "        for comment in submission.comments:\n",
    "            head_comments.append([comment.score,comment.body])\n",
    "            \n",
    "        if count%20==0:\n",
    "            print \"  Finished\",count,\"Out of\",Number_of_Posts\n",
    "            print \"    Time Elapsed:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "        subreddit_posts[post_id]=[post_title,str(post_date),post_score,post_url,head_comments,post_comments]\n",
    "    with open(json_name, 'w') as fp:\n",
    "        json.dump(subreddit_posts, fp)\n",
    "    \n",
    "    print \"Time Spent:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "    print \"Finished Collection from r/\"+subreddit_list[i][3:]\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import pprint\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import nltk\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, silhouette_score,classification_report\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, LinearRegression\n",
    "from sklearn import cluster, datasets, preprocessing, metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddit_names=pd.read_csv('Top Subreddits.csv')\n",
    "subreddit_list=['Subreddit Data/'+x[3:]+'.json' for x in subreddit_names['Reddit']]\n",
    "df=pd.read_json(subreddit_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_words(word_list):\n",
    "    cvec = TfidfVectorizer()\n",
    "    newtemp=pd.DataFrame()\n",
    "    df_list=[]\n",
    "    temp_word_list=[]\n",
    "    print \"Starting Word Vectorization:\",time.ctime()\n",
    "    \n",
    "    #---\n",
    "    for test in word_list[1]:\n",
    "        all_words=\"\"\n",
    "        for text in test:\n",
    "            all_words+=text+\" \"\n",
    "        all_words=all_words.replace(\"u'\",\"'\")\n",
    "        all_words=all_words.replace('\\\\n',\" \")\n",
    "        all_words=all_words.replace(\"\\\\\",\"\")\n",
    "        all_words=all_words.replace(\"\\'\",\"\")\n",
    "        if len(all_words)>5:\n",
    "            new_words=[word for word in all_words.split(\" \") if word not in stoppers]\n",
    "            new_words=[word for word in new_words]# if word not in stoppers\n",
    "            temp_string=\"\"\n",
    "            for word in new_words:\n",
    "                temp_string+=word+\" \"\n",
    "        temp_word_list.append(temp_string)\n",
    "    #---\n",
    "    \n",
    "    temp=pd.DataFrame(cvec.fit_transform(temp_word_list).todense(),columns=cvec.get_feature_names())\n",
    "#     temp[\"Mean\"]=np.mean(temp.iloc[:,:])\n",
    "#     temp2=temp[\"Mean\"].copy()\n",
    "#     for col in temp.columns[:-1]:\n",
    "#         temp2[col]=temp[col].copy()\n",
    "#     temp.sort_values(\"Mean\")\n",
    "    print \"Finished Word Vectorization:\",time.ctime()\n",
    "    print\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category_lists=[\"aggression.csv\", \"anarchy.json\", \"animals.csv\", \"art_and_literature.csv\", \"cars.csv\", \\\n",
    "                \"conservative_terms.json\", \"controversial_terms.json\", \"drugs.json\", \"eco_friendly.csv\",\\\n",
    "                \"environmental.json\", \"feminine_terms.csv\", \"fitness.json\", \"gaming.json\", \"liberal_terms.json\",\\\n",
    "                \"marxism.json\", \"music_and_television.csv\", \"outdoors_words.csv\", \"romantic_words.csv\",\\\n",
    "                \"sadness_sorry.csv\", \"sexual_terms.json\", \"sports.json\", \"technical_terms.json\", \\\n",
    "                \"theological.json\", \"video_games.csv\", \"vulgar_terms.json\", \"weeaboo.json\"]\n",
    "category_lists2=[\"aggression.csv\", \"animals.csv\", \"art_and_literature.csv\", \"cars.csv\", \\\n",
    "                \"conservative_terms.json\", \"controversial_terms.json\", \"drugs.json\", \"eco_friendly.csv\",\\\n",
    "                \"environmental.json\", \"feminine_terms.csv\", \"fitness.json\", \"gaming.json\", \"liberal_terms.json\",\\\n",
    "                \"marxism.json\", \"music_and_television.csv\", \"outdoors_words.csv\", \"romantic_words.csv\",\\\n",
    "                \"sadness_sorry.csv\", \"sexual_terms.json\", \"sports.json\", \"technical_terms.json\", \\\n",
    "                \"theological.json\", \"video_games.csv\", \"vulgar_terms.json\", \"weeaboo.json\"]\n",
    "base_extension=\"/Users/thomas/GA-DSI/projects/projects-capstone/Work 2, Personalities/Key Terms Lists/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we are just reading in the category word lists into a list. Out put is a list of category lists.\n",
    "i=0\n",
    "category_word_list=[]\n",
    "for i in range(len(category_lists)):\n",
    "    extension=base_extension+category_lists[i]\n",
    "    if category_lists[i][-3:]=='csv':\n",
    "        df=pd.read_csv(extension)\n",
    "        words=[word for word in df.iloc[:,0]]\n",
    "    elif category_lists[i]=='sexual_terms.json':\n",
    "        df=pd.read_json(extension)\n",
    "        words=[word for word in df.iloc[:,0]]\n",
    "    else:\n",
    "        df=pd.read_table(extension)\n",
    "        info=df.columns[0]\n",
    "        info=info.strip(':').strip(',').split('\\\\\"')\n",
    "        words=[]\n",
    "        for i in range(1,len(info)):\n",
    "            if i%2==0:\n",
    "                if len(info[i])>4:\n",
    "                    words.append(info[i])\n",
    "    category_word_list.append(words)\n",
    "category_count=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words=[]\n",
    "new_cat_word_list=[]\n",
    "i=0\n",
    "for words in category_word_list:\n",
    "    new_cat_word_list.append([x.lower() for x in words])\n",
    "    cat_words=pd.DataFrame(new_cat_word_list[-1]).iloc[:,0].value_counts()\n",
    "    del_words=[]\n",
    "    for word in range(len(cat_words)):\n",
    "            # This number below will remove words from a cat_list if they occur more than 'n' times\n",
    "        if cat_words[word]>1:\n",
    "            if cat_words[word] not in del_words:\n",
    "                del_words.append(cat_words.index[word])\n",
    "    for word in del_words:\n",
    "        while word in new_cat_word_list[-1]:\n",
    "            new_cat_word_list[-1].remove(word)\n",
    "category_word_list=new_cat_word_list\n",
    "for words in category_word_list:\n",
    "    all_words.extend(words)\n",
    "words=pd.DataFrame(all_words).iloc[:,0].value_counts()\n",
    "del_words=[]\n",
    "for word in range(len(words)):\n",
    "        #This number will remove words that occur across categories more than 'n' number of times\n",
    "    if words[word]>1:\n",
    "        del_words.append(words.index[word])\n",
    "for word in del_words:\n",
    "    for cat in category_word_list:\n",
    "        while word in cat:\n",
    "            cat.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here, we are reading in our post information in the format ([post_type_list],[post_comments_list]).\n",
    "# Output is a list of subreddits, which contains a list of all post information.\n",
    "subreddit_names=pd.read_csv('Top Subreddits.csv')\n",
    "subreddit_list=['Subreddit Data/'+x[3:]+'.json' for x in subreddit_names['Reddit']]\n",
    "df=pd.read_json(subreddit_list[0])\n",
    "top_post_info=[]\n",
    "count=0\n",
    "start_time=time.time()\n",
    "for subreddit in subreddit_list:\n",
    "    if count%20==0:\n",
    "        print \n",
    "    if count<9:\n",
    "        print \"\",count+1,\" \",\n",
    "    else:\n",
    "        print count+1,\" \",\n",
    "    count+=1\n",
    "    post_comments=[]\n",
    "    post_types=[]\n",
    "    subreddit_df=pd.read_json(subreddit)\n",
    "    for post in subreddit_df.columns:\n",
    "        comments=[]\n",
    "        for x in re.findall(r\"(u'comment': )(.*)|(\\n*)(,\\n\\s*u'id': )\",pprint.pformat(subreddit_df.loc[5,post])):\n",
    "            if \"u'id': u\" in x[1]:\n",
    "                comments.append(x[1][:-18])\n",
    "            else:\n",
    "                comments.append(x[1])\n",
    "        post_comments.append(comments)\n",
    "        post_types.append(subreddit_df.loc[3,post].split('.')[-1].split('/')[0][:3])\n",
    "    top_post_info.append([post_types,post_comments])\n",
    "print\n",
    "print 'Took %s seconds'%(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### We will use this later on in the clustering; we put this here because it is relevant\n",
    "all_post_types=[]\n",
    "for x in top_post_info:\n",
    "    all_post_types.append([])\n",
    "    for y in x[0]:\n",
    "        if y in ['com', 'jpg', 'gif', 'png', 'htm']:\n",
    "            all_post_types[-1].append(y)\n",
    "        else:\n",
    "            all_post_types[-1].append('com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our function will take in the subreddit data, clean the comments to get individual words, and then will \n",
    "# check all of the words in the comments of a post to create a tally of how many category words were mentioned.\n",
    "# This post-category_word_count matrix will be done for every post within a subreddit. The function will then\n",
    "# return the matrix of category_word_counts for each category for each post within a subreddit.\n",
    "\n",
    "def find_subreddit_categories(word_list,curr_title):\n",
    "    newtemp=pd.DataFrame()\n",
    "    df_list=[]\n",
    "    temp_word_list=[]\n",
    "    print \"Starting Word Vectorization #%s at:\"%curr_title,time.ctime()\n",
    "    \n",
    "    #---\n",
    "    for test in word_list[1]:\n",
    "        new_words=[]\n",
    "        all_words=\"\"\n",
    "        for text in test:\n",
    "            all_words+=text+\" \"\n",
    "        all_words=all_words.replace(\"u'\",\"'\")\n",
    "        all_words=all_words.replace('\\\\n',\" \")\n",
    "        all_words=all_words.replace(\"\\\\\",\"\")\n",
    "        all_words=all_words.replace(\"\\'\",\"\")\n",
    "        if len(all_words)>5:\n",
    "            new_words=[word for word in all_words.split(\" \") if word not in stoppers]\n",
    "        temp_word_list.append(new_words)\n",
    "    #---\n",
    "    counter=0\n",
    "    # temp_word_list has len 100 going into this\n",
    "    category_count=[]\n",
    "    for post_word_list in temp_word_list:\n",
    "        counter+=1\n",
    "        if counter%75==0:\n",
    "            print \"Running Number\",counter,\"of 100\"\n",
    "        category_count.append([])\n",
    "        for category_index in range(len(category_word_list)):\n",
    "            cat_count=0\n",
    "            for word in category_word_list[category_index]:\n",
    "                cat_count+=post_word_list.count(word)\n",
    "            category_count[-1].append([])\n",
    "            category_count[-1][category_index]=cat_count\n",
    "    temp=pd.DataFrame(category_count)\n",
    "    temp.columns=category_lists\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here, we use the function defined above to get the category_word_count matrix for each subreddit. We put \n",
    "# those matrices into a list. That list is then pickled to save time.\n",
    "i=0\n",
    "category_dfs=[]\n",
    "for top_posts in top_post_info:\n",
    "    i+=1\n",
    "    category_dfs.append(find_subreddit_categories(top_posts,i))\n",
    "pickle.dump(category_dfs,open('pickled_category_dfs','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "85\n",
      "\n",
      "Starting Collection from /r/YouShouldKnow\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.68096050024 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.33836431503 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.00325208108 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.66899548372 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.33692808151 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.03633249998 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.729213051 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.39169801474 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.05561861595 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.72417749961 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.42403908173 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.09093815088 minutes\n",
      "Time Spent: 8.43230814934 minutes\n",
      "Finished Collection from r/YouShouldKnow\n",
      "\n",
      "\n",
      "86\n",
      "\n",
      "Starting Collection from /r/nfl\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.733790250619 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.40975675186 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.05808695157 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.72775926987 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.40048288504 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.09172865152 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.76171860298 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.42964883645 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.09303327004 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.75668075085 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.45874288479 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.13634156783 minutes\n",
      "Time Spent: 8.49336825212 minutes\n",
      "Finished Collection from r/nfl\n",
      "\n",
      "\n",
      "87\n",
      "\n",
      "Starting Collection from /r/AskHistorians\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.701740980148 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.37183383306 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.03695611556 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.70216346582 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.36901906331 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.06940019925 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.73635154963 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.4088493824 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.06826923291 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.73486868143 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.43582158089 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.10211193164 minutes\n",
      "Time Spent: 8.43964099884 minutes\n",
      "Finished Collection from r/AskHistorians\n",
      "\n",
      "\n",
      "88\n",
      "\n",
      "Starting Collection from /r/HistoryPorn\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.734015349547 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.39972926776 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.06325178544 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.74795360168 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.41091618538 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.11191421747 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.79038776557 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.46098475059 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.19205911557 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.85872013569 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.57520456711 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.24494940042 minutes\n",
      "Time Spent: 8.56390943527 minutes\n",
      "Finished Collection from r/HistoryPorn\n",
      "\n",
      "\n",
      "89\n",
      "\n",
      "Starting Collection from /r/lifehacks\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.730324884256 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.3942157348 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.08162266811 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.73411208391 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.39438488483 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.12638486624 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.76980090141 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.44367750088 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.12802981933 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.77377420266 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.47814979951 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.1374993523 minutes\n",
      "Time Spent: 8.49458529949 minutes\n",
      "Finished Collection from r/lifehacks\n",
      "\n",
      "\n",
      "90\n",
      "\n",
      "Starting Collection from /r/fffffffuuuuuuuuuuuu\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.709685981274 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.38357406457 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.04752115011 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.71467468341 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.3881663998 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.07909988165 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.7429845651 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.41856724819 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.07860405048 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.74229969978 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.44571784735 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.10751804908 minutes\n",
      "Time Spent: 8.4447876811 minutes\n",
      "Finished Collection from r/fffffffuuuuuuuuuuuu\n",
      "\n",
      "\n",
      "91\n",
      "\n",
      "Starting Collection from /r/soccer\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.734490450223 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.40233286619 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.06857093175 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.73529149691 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.40090058247 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.10938441356 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.77143939734 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.46335846583 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.10368929704 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.7653929472 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.47289788326 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.14759654999 minutes\n",
      "Time Spent: 8.47596063217 minutes\n",
      "Finished Collection from r/soccer\n",
      "\n",
      "\n",
      "92\n",
      "\n",
      "Starting Collection from /r/OutOfTheLoop\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.725416600704 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.39125684897 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.05635329882 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.72426429987 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.39044214884 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.08999493122 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.75622538328 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.42429026763 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.09052446683 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.75641280015 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.45706249873 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.14180341562 minutes\n",
      "Time Spent: 8.45940599839 minutes\n",
      "Finished Collection from r/OutOfTheLoop\n",
      "\n",
      "\n",
      "93\n",
      "\n",
      "Starting Collection from /r/comics\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.72920598189 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.41461799939 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.06519236565 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.72991319895 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.39669868151 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.09666333199 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.76437268257 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.43075201511 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.11710353295 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.76333904664 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.46414463123 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.13512779872 minutes\n",
      "Time Spent: 8.46483613253 minutes\n",
      "Finished Collection from r/comics\n",
      "\n",
      "\n",
      "94\n",
      "\n",
      "Starting Collection from /r/StarWars\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.734859883785 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.39898381631 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.07002405326 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.73247003555 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.40471434991 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.10024731954 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.76617016792 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.43357661963 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.10058613618 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.76907620033 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.47316425244 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.14070663452 minutes\n",
      "Time Spent: 8.47089516719 minutes\n",
      "Finished Collection from r/StarWars\n",
      "\n",
      "\n",
      "95\n",
      "\n",
      "Starting Collection from /r/tattoos\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.729060848554 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.39585994879 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.066573898 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.72853271564 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.39471611579 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.09466034969 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.7616847833 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.42998698155 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.09697816372 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.76225514809 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.46152374744 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.13040613333 minutes\n",
      "Time Spent: 8.46289009651 minutes\n",
      "Finished Collection from r/tattoos\n",
      "\n",
      "\n",
      "96\n",
      "\n",
      "Starting Collection from /r/nba\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.733601164818 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.40787533124 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.07518306573 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.73780291478 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.4034555157 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.11941083272 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.78174538215 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.46168771585 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.10463054975 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.77382753293 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.47186113199 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.1489286979 minutes\n",
      "Time Spent: 8.48128033479 minutes\n",
      "Finished Collection from r/nba\n",
      "\n",
      "\n",
      "97\n",
      "\n",
      "Starting Collection from /r/me_irl\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.739106667042 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.40394866467 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.05526891549 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.72405784925 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.41323073308 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.09760118326 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.75490183433 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.42165906827 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.08886761665 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.75517048438 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.45522713264 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.12516951561 minutes\n",
      "Time Spent: 8.46047536532 minutes\n",
      "Finished Collection from r/me_irl\n",
      "\n",
      "\n",
      "98\n",
      "\n",
      "Starting Collection from /r/cringe\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.733938467503 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.39694771767 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.06467074951 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.73271069924 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.39783996741 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.09596680005 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.76710948149 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.4288056016 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.09561081727 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.77616928418 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.46695679824 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.13103558222 minutes\n",
      "Time Spent: 8.46767021815 minutes\n",
      "Finished Collection from r/cringe\n",
      "\n",
      "\n",
      "99\n",
      "\n",
      "Starting Collection from /r/facepalm\n",
      "  Finished 20 Out of 250\n",
      "    Time Elapsed: 0.727019266287 minutes\n",
      "  Finished 40 Out of 250\n",
      "    Time Elapsed: 1.39549513261 minutes\n",
      "  Finished 60 Out of 250\n",
      "    Time Elapsed: 2.06618194977 minutes\n",
      "  Finished 80 Out of 250\n",
      "    Time Elapsed: 2.74258783261 minutes\n",
      "  Finished 100 Out of 250\n",
      "    Time Elapsed: 3.40188968579 minutes\n",
      "  Finished 120 Out of 250\n",
      "    Time Elapsed: 4.10267325242 minutes\n",
      "  Finished 140 Out of 250\n",
      "    Time Elapsed: 4.77343925238 minutes\n",
      "  Finished 160 Out of 250\n",
      "    Time Elapsed: 5.43934328556 minutes\n",
      "  Finished 180 Out of 250\n",
      "    Time Elapsed: 6.10352761745 minutes\n",
      "  Finished 200 Out of 250\n",
      "    Time Elapsed: 6.76924066544 minutes\n",
      "  Finished 220 Out of 250\n",
      "    Time Elapsed: 7.46961053212 minutes\n",
      "  Finished 240 Out of 250\n",
      "    Time Elapsed: 8.13670111895 minutes\n",
      "Time Spent: 8.47611575127 minutes\n",
      "Finished Collection from r/facepalm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('Top Subreddits.csv')\n",
    "subreddit_list=[x for x in df['Reddit']]\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "Number_of_Posts=250\n",
    "Number_of_Comments=0\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "# for i in range(24,len(subreddit_list)):\n",
    "#     print \n",
    "#     print i\n",
    "#     print \n",
    "#     count=0\n",
    "#     current_time=time.time()\n",
    "#     print \"Starting Collection from \"+subreddit_list[i]\n",
    "#     subreddit_posts={}\n",
    "#     json_name='/Users/thomas/GA-DSI/projects/projects-capstone/Work 2, Personalities/Hot/'\\\n",
    "#                         +subreddit_list[i][3:]+'.json'\n",
    "#     info_for_request='get top '+str(Number_of_Posts)+' posts w/ top '+str(Number_of_Comments)+\\\n",
    "#                         ' comments and children comments from '+subreddit_list[i]+\\\n",
    "#                         \"for Capstone project at General Assembly's DSI program by User: xxx\"\n",
    "#     r = praw.Reddit(user_agent=info_for_request)\n",
    "#     r.login('xxx', 'xxx',disable_warning=True)\n",
    "#     posts = r.get_subreddit(subreddit_list[i][3:]).get_hot(limit=Number_of_Posts)\n",
    "#     for post in posts:\n",
    "#         count+=1\n",
    "#         post_comments=[]\n",
    "#         head_comments=[]\n",
    "        \n",
    "#         post_id=post.id\n",
    "#         post_title=post.title\n",
    "#         post_score=post.score\n",
    "#         post_url=post.url\n",
    "#         post_date=datetime.datetime.fromtimestamp(post.created)\n",
    "# #         post_author=post.author\n",
    "        \n",
    "#         submission = r.get_submission(submission_id=post_id)\n",
    "#         submission.replace_more_comments(limit=Number_of_Comments, threshold=0)\n",
    "#         for comment in submission.comments:\n",
    "#             \"\"\" loops through all comments to submission \"\"\"\n",
    "#             x = []  # array that holds the comment and all replies\n",
    "#             # get all replies and append it to our array for writing to file\n",
    "#             post_comments.append(get_replies(comment, x))\n",
    "#         for comment in submission.comments:\n",
    "#             head_comments.append([comment.score,comment.body])\n",
    "            \n",
    "#         if count%20==0:\n",
    "#             print \"  Finished\",count,\"Out of\",Number_of_Posts\n",
    "#             print \"    Time Elapsed:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "#         subreddit_posts[post_id]=[post_title,str(post_date),post_score,post_url,head_comments,post_comments]\n",
    "#     with open(json_name, 'w') as fp:\n",
    "#         json.dump(subreddit_posts, fp)\n",
    "    \n",
    "#     print \"Time Spent:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "#     print \"Finished Collection from r/\"+subreddit_list[i][3:]\n",
    "#     print\n",
    "    \n",
    "for i in range(85,len(subreddit_list)):\n",
    "    print \n",
    "    print i\n",
    "    print \n",
    "    count=0\n",
    "    current_time=time.time()\n",
    "    print \"Starting Collection from \"+subreddit_list[i]\n",
    "    subreddit_posts={}\n",
    "    json_name='/Users/thomas/GA-DSI/projects/projects-capstone/Work 2, Personalities/Controversial/'\\\n",
    "                        +subreddit_list[i][3:]+'.json'\n",
    "    info_for_request='get top '+str(Number_of_Posts)+' posts w/ top '+str(Number_of_Comments)+\\\n",
    "                        ' comments and children comments from '+subreddit_list[i]+\\\n",
    "                        \"for Capstone project at General Assembly's DSI program by User: xxx\"\n",
    "    r = praw.Reddit(user_agent=info_for_request)\n",
    "    r.login('xxx', 'xxx',disable_warning=True)\n",
    "    posts = r.get_subreddit(subreddit_list[i][3:]).get_controversial_from_year(limit=Number_of_Posts)\n",
    "    for post in posts:\n",
    "        count+=1\n",
    "        post_comments=[]\n",
    "        head_comments=[]\n",
    "        \n",
    "        post_id=post.id\n",
    "        post_title=post.title\n",
    "        post_score=post.score\n",
    "        post_url=post.url\n",
    "        post_date=datetime.datetime.fromtimestamp(post.created)\n",
    "#         post_author=post.author\n",
    "        \n",
    "        submission = r.get_submission(submission_id=post_id)\n",
    "        submission.replace_more_comments(limit=Number_of_Comments, threshold=0)\n",
    "        for comment in submission.comments:\n",
    "            \"\"\" loops through all comments to submission \"\"\"\n",
    "            x = []  # array that holds the comment and all replies\n",
    "            # get all replies and append it to our array for writing to file\n",
    "            post_comments.append(get_replies(comment, x))\n",
    "        for comment in submission.comments:\n",
    "            head_comments.append([comment.score,comment.body])\n",
    "            \n",
    "        if count%20==0:\n",
    "            print \"  Finished\",count,\"Out of\",Number_of_Posts\n",
    "            print \"    Time Elapsed:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "        subreddit_posts[post_id]=[post_title,str(post_date),post_score,post_url,head_comments,post_comments]\n",
    "    with open(json_name, 'w') as fp:\n",
    "        json.dump(subreddit_posts, fp)\n",
    "    \n",
    "    print \"Time Spent:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "    print \"Finished Collection from r/\"+subreddit_list[i][3:]\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-12-6ebc6bc7b4ce>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-6ebc6bc7b4ce>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    current_time=time.time()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# for i in range(,len(user_list)):\n",
    "    current_time=time.time()\n",
    "    print \"Starting Collection from user %i out of %s users\" %i, %len(user_list) \n",
    "    user_info={}\n",
    "    json_name='/Users/thomas/GA-DSI/projects/projects-capstone/Work 2, Personalities/User Data/'\\\n",
    "                        +user_list[i]+'.json'\n",
    "    info_for_request='get info on user'+str(user_list[i])+\\\n",
    "                        'posts, likes, and dislikes'\\\n",
    "                        \"for Capstone project at General Assembly's DSI program by User: xxx\"\n",
    "    r = praw.Reddit(user_agent=info_for_request)\n",
    "    r.login('xxx', 'xxx',disable_warning=True)\n",
    "\n",
    "    user=r.get_redditor(user_list[i])\n",
    "    comments=[]\n",
    "    liked=[]\n",
    "    disliked=[]\n",
    "    submitted=[]\n",
    "    for x in user.get_comments(limit=None,time='All'):\n",
    "        comments.append(x) #Get scores if possible\n",
    "    for x in user.get_liked():\n",
    "        liked.append(x)\n",
    "    for x in user.get_disliked():\n",
    "        disliked.append(x)\n",
    "    for x in user.get_submitted():\n",
    "        submitted.append(x)\n",
    "\n",
    "    if i%20==0:\n",
    "            print \"  Finished\",count,\"Out of\",len(user_list)\n",
    "            print \"    Time Elapsed:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "    user_info[user_list[i]]=[comments,liked,disliked,submitted]\n",
    "    with open(json_name, 'w') as fp:\n",
    "        json.dump(user_info, fp)\n",
    "    \n",
    "    print \"Time Spent:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "    print \"Finished Collection from r/\"+subreddit_list[i][3:]\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
