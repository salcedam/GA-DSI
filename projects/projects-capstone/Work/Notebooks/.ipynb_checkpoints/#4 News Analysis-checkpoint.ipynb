{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier,RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from nltk.stem import SnowballStemmer,PorterStemmer\n",
    "from stemming import lovins,porter2,paicehusk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import stop_words\n",
    "import pprint\n",
    "import re\n",
    "import time\n",
    "import spacy\n",
    "import datetime\n",
    "import requests\n",
    "import BeautifulSoup\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting JSON Loading\n",
      "Loading JSON at Time: 1.58150990804e-06 Minutes\n",
      "Loading JSON at Time: 0.955058598518 Minutes\n",
      "Loading JSON at Time: 2.19378606478 Minutes\n",
      "Loading JSON at Time: 3.52724283139 Minutes\n",
      "Loading JSON at Time: 3.82851681709 Minutes\n",
      "Loading JSON at Time: 4.97993968328 Minutes\n",
      "Loading JSON at Time: 6.52209508419 Minutes\n",
      "Finished JSON Loading\n",
      "Took: 7.38683884939 Minutes\n",
      "\n",
      "Starting DF reformatting\n",
      "Finished DF reformatting\n",
      "Took: 0.862464189529 Seconds\n"
     ]
    }
   ],
   "source": [
    "### THIS CODE LOADS THE JSONS OF THE VECTORIZED REDDIT COMMENTS, AND GETS IT TO ITS PRIOR USABLE STATE\n",
    "total_time=time.time()\n",
    "print \"Starting Reddit DF Loading\"\n",
    "start_time=time.time()\n",
    "news=pd.read_json('../r_news.json')\n",
    "worldnews=pd.read_json('../r_worldnews.json')\n",
    "tech=pd.read_json('../r_technology.json')\n",
    "stock=pd.read_json('../r_StockMarket.json')\n",
    "politics=pd.read_json('../r_politics.json')\n",
    "inthenews=pd.read_json('../r_inthenews.json')\n",
    "futurology=pd.read_json('../r_Futurology.json')\n",
    "dfs={'news':news,'worldnews':worldnews,'tech':tech,'stock':stock,'politics':politics,\\\n",
    "     'inthenews':inthenews,'futurology':futurology}\n",
    "for name in dfs.keys():\n",
    "    df=dfs[name]\n",
    "    for i in range(len(df.columns)):\n",
    "        all_words=pprint.pformat(df.iloc[4,i])\n",
    "        all_words=all_words.replace(\"\\n\",\" \").replace(\"\\\\n\",\" \").replace(\"{u'comment': \",\"\")\\\n",
    "                .replace(\",\",\"\").replace(\"ucomment: \",\"\")\n",
    "        all_words=re.sub(r\"[\\[\\]]+\",\" \",all_words)\n",
    "        all_words=re.sub(r\"\\Wu\\'id\\': \\w\\'\\w{7}\\'\",\"\",all_words).replace(\"{\",\"\").replace(\"}\",\"\").replace(\"      \",\" \")\\\n",
    "                    .replace(\"\\\\\\'\",\"\\'\").replace(\"' u\\'\",\" \").replace(\" u\\'\",\"\")\n",
    "        all_words=re.sub(r\"\\\\u20..\",\"\",all_words).replace(\"\\'\",\"\").replace(' u\"',\"\")\n",
    "        all_words=re.sub(r\"[A-Za-z]*[0-9]+[A-Za-z]*\",\"\",all_words).replace(\"*\",\"\")\n",
    "        df.iloc[4,i]=all_words\n",
    "    dfs[name]=df\n",
    "print \"Finished Reddit DF Loading\"\n",
    "print \"Took %s Minutes\"%((time.time()-start_time)/60.)\n",
    "print \n",
    "\n",
    "jsons=[]\n",
    "json_dfs=[]\n",
    "print \"Starting Vectorized Words Loading\"\n",
    "start_time=time.time()\n",
    "for link in ['../vectorized_r_news.json','../vectorized_r_worldnews.json','../vectorized_r_technology.json',\\\n",
    "            '../vectorized_r_StockMarket.json','../vectorized_r_politics.json','../vectorized_r_inthenews.json',\\\n",
    "            '../vectorized_r_Futurology.json']:\n",
    "    jsons.append(pd.read_json(link))\n",
    "\n",
    "start_time=time.time()\n",
    "for i in range(len(jsons)):\n",
    "    temp_df=jsons[i]\n",
    "    temp_df=temp_df.T.sort_values(\"Mean\",ascending=False)\n",
    "    del temp_df[\"Mean\"]\n",
    "    new_cols=[]\n",
    "    for col in temp_df.columns:\n",
    "        new_cols.append(int(col))\n",
    "    temp_df.columns=new_cols\n",
    "    temp_df[\"Mean\"]=temp_df.T.mean()\n",
    "    temp_df=temp_df.sort_values(\"Mean\",ascending=False).T.sort_index()\n",
    "    json_dfs.append(temp_df)\n",
    "print \"Finished Vectorized Words Loading\"\n",
    "print \"Took: %s Minutes\" %((time.time()-start_time)/60.)\n",
    "print\n",
    "\n",
    "###---\n",
    "\n",
    "start_time=time.time()\n",
    "print \"Starting Fed, Keywords, and Pos/Neg\"\n",
    "fed=pd.read_json('../fed_info.json')\n",
    "del fed['f2015043']\n",
    "del fed['f2015042']\n",
    "cols=[]\n",
    "for col in fed.columns:\n",
    "    if col[0]!='f':\n",
    "        name=col[:4]+'-'+col[4:6]+'-'+col[6:]\n",
    "        cols.append(name)\n",
    "    else: print col\n",
    "cols=pd.to_datetime(cols)\n",
    "fed.columns=cols\n",
    "# We eliminated columns before the 119th because we want to focus in on a 1 year period. We state the end of the \n",
    "# period is the last date in our data set\n",
    "fed=fed.iloc[:,119:]\n",
    "keywords=pd.read_json('../key_terms.json').reset_index()\n",
    "keywords['index']=keywords['index'].astype(int)\n",
    "keywords=keywords.sort_values('index')\n",
    "keywords.index=keywords['index']\n",
    "del keywords['index']\n",
    "keywords.columns=['Type','Value']\n",
    "keywords[\"Value\"]=keywords[\"Value\"].apply(lambda x:x.lower())\n",
    "\n",
    "posneg=pd.read_csv('../subjectivity of words.txt')\n",
    "word_list=['type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative'.split(\" \")]\n",
    "posneg.columns=['col']\n",
    "for row in posneg['col']:\n",
    "    word_list.append(row.split(\" \"))\n",
    "posneg=[]\n",
    "for x in word_list:\n",
    "    posneg.append([])\n",
    "    for y in x:\n",
    "        posneg[-1].append(y.split(\"=\")[-1])\n",
    "posneg=pd.DataFrame(posneg)\n",
    "posneg.columns=['type','len','word','pos','stemmed','priorpolarity','N/A']\n",
    "print \"Finished Fed, Keywords, Pos/Neg\"\n",
    "print \"Took: %s Minutes\" %((time.time()-start_time())/60.)\n",
    "print \"\\n \\n\"\n",
    "print \"Total time: %s Minutes\"%((time.time()-total_time)/60.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reddit DF Loading\n",
      "Finished Reddit DF Loading\n",
      "Took: 86.6783220768\n"
     ]
    }
   ],
   "source": [
    "print \"Starting Reddit DF Loading\"\n",
    "start_time=time.time()\n",
    "news=pd.read_json('../r_news.json')\n",
    "worldnews=pd.read_json('../r_worldnews.json')\n",
    "tech=pd.read_json('../r_technology.json')\n",
    "stock=pd.read_json('../r_StockMarket.json')\n",
    "politics=pd.read_json('../r_politics.json')\n",
    "inthenews=pd.read_json('../r_inthenews.json')\n",
    "futurology=pd.read_json('../r_Futurology.json')\n",
    "dfs={'news':news,'worldnews':worldnews,'tech':tech,'stock':stock,'politics':politics,\\\n",
    "     'inthenews':inthenews,'futurology':futurology}\n",
    "for name in dfs.keys():\n",
    "    df=dfs[name]\n",
    "    for i in range(len(df.columns)):\n",
    "        all_words=pprint.pformat(df.iloc[4,i])\n",
    "        all_words=all_words.replace(\"\\n\",\" \").replace(\"\\\\n\",\" \").replace(\"{u'comment': \",\"\")\\\n",
    "                .replace(\",\",\"\").replace(\"ucomment: \",\"\")\n",
    "        all_words=re.sub(r\"[\\[\\]]+\",\" \",all_words)\n",
    "        all_words=re.sub(r\"\\Wu\\'id\\': \\w\\'\\w{7}\\'\",\"\",all_words).replace(\"{\",\"\").replace(\"}\",\"\").replace(\"      \",\" \")\\\n",
    "                    .replace(\"\\\\\\'\",\"\\'\").replace(\"' u\\'\",\" \").replace(\" u\\'\",\"\")\n",
    "        all_words=re.sub(r\"\\\\u20..\",\"\",all_words).replace(\"\\'\",\"\").replace(' u\"',\"\")\n",
    "        all_words=re.sub(r\"[A-Za-z]*[0-9]+[A-Za-z]*\",\"\",all_words).replace(\"*\",\"\")\n",
    "        df.iloc[4,i]=all_words\n",
    "    dfs[name]=df\n",
    "print \"Finished Reddit DF Loading\"\n",
    "print \"Took:\",time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary of all the words, and link the word to a keyword identity and strength. \n",
    "It might be smart to take a series of documents and find out how often each of the keysords appears. From this, one could deduce an implied strength by their relevance in the articles. Financial articles would be the best choice for this. (Maybe scrape Yahoo finance?)\n",
    "\n",
    "Categorize the post to the identities and give them a strength.\n",
    "    \n",
    "* There would be a column for each type of keyword. The value of the column would be the sum of the keywords with that category. The algorithms would weight each of those values linearly, so scale them appropriately before. This would be done just after the vectorization process.\n",
    "    \n",
    "The next step is to introduce the sentiment implied by the comments and the direction of this sentiment. It might be smart to use only the top posts, and none of their child comments (they can get pretty stupid.) However, there are some subreddits with sane commentors; we wouldn't necessarily need to filter these (especially as they will probably have fewer comments).\n",
    "\n",
    "After the vectorized sentiment has been calculated, we will have keywords to identify them with. If the keywords in the comments match what is in the title, then it is safe to assume an implied investor sentiment; this will be a column. (Here, it might be smart to create derivative columns consisting of the implied sentiment column and the identity columns).\n",
    "\n",
    "From here, we can begin to look at classifications, regressions, and clustering.\n",
    "\n",
    "We want to predict $\\Delta$Price, and reset the price on the day after we think it is going to be realized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Go through all of the fed articles and see:\n",
    "\n",
    "    1) How many times each type of keyword occurs \n",
    "    2) How many times each keyword appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type_dict_pos={}\n",
    "for word in keywords['Type']:\n",
    "    type_dict_pos[word]=0\n",
    "type_dict_neg=type_dict_pos.copy()\n",
    "type_dict_gen=type_dict_pos.copy()\n",
    "word_dict_pos={}\n",
    "for word in keywords['Value']:\n",
    "    word_dict_pos[word]=0\n",
    "word_dict_neg=word_dict_pos.copy()\n",
    "word_dict_gen=word_dict_pos.copy()\n",
    "\n",
    "\n",
    "\n",
    "for col in fed.columns:\n",
    "    title=fed.loc[0,col]\n",
    "    pos_count=0\n",
    "    neg_count=0\n",
    "    for word in posneg['word']:\n",
    "        if word in title:\n",
    "            sentiment=posneg[posneg['word']==word]['priorpolarity'].values[0]\n",
    "            if sentiment=='negative':\n",
    "                neg_count+=1\n",
    "            elif sentiment=='positive':\n",
    "                pos_count+=1\n",
    "    article=\"\"\n",
    "    for x in fed.loc[2,col]:\n",
    "        article+=x.lower()+' '\n",
    "    if pos_count>neg_count:\n",
    "        for word in keywords['Value']:\n",
    "            if word in article:\n",
    "                word_dict_pos[word]+=1\n",
    "                type_dict_pos[keywords[keywords['Value']==word]['Type'].values[0]]+=1\n",
    "    elif pos_count<neg_count:\n",
    "        for word in keywords['Value']:\n",
    "            if word in article:\n",
    "                word_dict_neg[word]+=1\n",
    "                type_dict_neg[keywords[keywords['Value']==word]['Type'].values[0]]+=1\n",
    "    else:\n",
    "        for word in keywords['Value']:\n",
    "            if word in article:\n",
    "                word_dict_gen[word]+=1\n",
    "                type_dict_gen[keywords[keywords['Value']==word]['Type'].values[0]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_results=pd.DataFrame([word_dict_neg,word_dict_pos,word_dict_gen]).T\n",
    "word_results.columns=['Negative','Positive',\"General\"]\n",
    "word_results[\"Occurences\"]=word_results[\"Positive\"]+word_results[\"Negative\"]+word_results[\"General\"]\n",
    "word_results[\"Rating\"]=(word_results[\"Positive\"]-word_results['Negative'])/word_results[\"Occurences\"]\n",
    "word_results[word_results['Occurences']>3].sort_values(\"Rating\")[[\"Occurences\",\"Rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occurences</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>iowa</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>georgia</th>\n",
       "      <td>40</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indiana</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maryland</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massachusetts</th>\n",
       "      <td>12</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minnesota</th>\n",
       "      <td>16</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>columbus</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>florida</th>\n",
       "      <td>69</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>california</th>\n",
       "      <td>518</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ohio</th>\n",
       "      <td>14</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missouri</th>\n",
       "      <td>24</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arizona</th>\n",
       "      <td>11</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alabama</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oklahoma</th>\n",
       "      <td>10</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kansas</th>\n",
       "      <td>35</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginia</th>\n",
       "      <td>40</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>san francisco</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trading</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <td>7</td>\n",
       "      <td>-0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kansas city</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>6</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>14</td>\n",
       "      <td>-0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicago</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accepted</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>department</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>million</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nation</th>\n",
       "      <td>120</td>\n",
       "      <td>-0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>media</th>\n",
       "      <td>118</td>\n",
       "      <td>-0.347458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bankrupt</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bankruptcy</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>equity</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rice</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rates</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>implementation</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pennsylvania</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paid</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>owed</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inflation</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colorado</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>columbia</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>federal register</th>\n",
       "      <td>48</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unity</th>\n",
       "      <td>19</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>18</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liquidity</th>\n",
       "      <td>6</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texas</th>\n",
       "      <td>114</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economy</th>\n",
       "      <td>5</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>authority</th>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan</th>\n",
       "      <td>8</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold</th>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goldman sachs</th>\n",
       "      <td>4</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>federal reserve system</th>\n",
       "      <td>6</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>north carolina</th>\n",
       "      <td>40</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loans</th>\n",
       "      <td>5</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>connecticut</th>\n",
       "      <td>6</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utah</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louisiana</th>\n",
       "      <td>5</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Occurences    Rating\n",
       "iowa                             8 -1.000000\n",
       "georgia                         40 -1.000000\n",
       "indiana                         10 -1.000000\n",
       "maryland                         4 -1.000000\n",
       "massachusetts                   12 -1.000000\n",
       "minnesota                       16 -1.000000\n",
       "columbus                         4 -1.000000\n",
       "florida                         69 -1.000000\n",
       "california                     518 -1.000000\n",
       "ohio                            14 -1.000000\n",
       "missouri                        24 -1.000000\n",
       "arizona                         11 -1.000000\n",
       "alabama                         10 -1.000000\n",
       "stock                            4 -1.000000\n",
       "oklahoma                        10 -1.000000\n",
       "kansas                          35 -0.800000\n",
       "virginia                        40 -0.800000\n",
       "san francisco                    4 -0.750000\n",
       "trading                          8 -0.750000\n",
       "city                             5 -0.600000\n",
       "exchange                         7 -0.571429\n",
       "kansas city                      4 -0.500000\n",
       "finance                          6 -0.500000\n",
       "government                      14 -0.428571\n",
       "chicago                          5 -0.400000\n",
       "accepted                         5 -0.400000\n",
       "department                       5 -0.400000\n",
       "million                          8 -0.375000\n",
       "nation                         120 -0.366667\n",
       "media                          118 -0.347458\n",
       "...                            ...       ...\n",
       "bankrupt                         4  0.000000\n",
       "bankruptcy                       4  0.000000\n",
       "equity                           8  0.000000\n",
       "rice                            11  0.000000\n",
       "rates                            5  0.000000\n",
       "implementation                  13  0.000000\n",
       "pennsylvania                    16  0.000000\n",
       "price                           11  0.000000\n",
       "paid                             6  0.000000\n",
       "owed                             7  0.000000\n",
       "inflation                       14  0.000000\n",
       "colorado                        26  0.000000\n",
       "debt                            12  0.000000\n",
       "columbia                         6  0.000000\n",
       "federal register                48  0.083333\n",
       "unity                           19  0.105263\n",
       "business                        18  0.111111\n",
       "liquidity                        6  0.166667\n",
       "texas                          114  0.333333\n",
       "economy                          5  0.400000\n",
       "authority                        6  0.500000\n",
       "loan                             8  0.500000\n",
       "gold                             4  0.500000\n",
       "goldman sachs                    4  0.500000\n",
       "federal reserve system           6  0.500000\n",
       "north carolina                  40  0.500000\n",
       "loans                            5  0.600000\n",
       "connecticut                      6  1.000000\n",
       "utah                             5  1.000000\n",
       "louisiana                        5  1.000000\n",
       "\n",
       "[98 rows x 2 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_results[word_results['Occurences']>3].sort_values(\"Rating\")[[\"Occurences\",\"Rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type_dict_pos={}\n",
    "for word in keywords['Type']:\n",
    "    type_dict_pos[word]=0\n",
    "type_dict_neg=type_dict_pos.copy()\n",
    "type_dict_gen=type_dict_pos.copy()\n",
    "word_dict_pos={}\n",
    "for word in keywords['Value']:\n",
    "    word_dict_pos[word]=0\n",
    "word_dict_neg=word_dict_pos.copy()\n",
    "word_dict_gen=word_dict_pos.copy()\n",
    "\n",
    "\n",
    "article_row=4\n",
    "for name in dfs.keys():\n",
    "    df=dfs[name]\n",
    "    for col in df.columns:\n",
    "        title=df.loc[0,col]\n",
    "        pos_count=0\n",
    "        neg_count=0\n",
    "        for word in posneg['word']:\n",
    "            if word in title:\n",
    "                sentiment=posneg[posneg['word']==word]['priorpolarity'].values[0]\n",
    "                if sentiment=='negative':\n",
    "                    neg_count+=1\n",
    "                elif sentiment=='positive':\n",
    "                    pos_count+=1\n",
    "    # Use for fed\n",
    "    #     article=\"\"\n",
    "    #     for x in df.loc[article_row,col]:\n",
    "    #         article+=x.lower()+' '\n",
    "        article=df.loc[article_row,col].lower()\n",
    "        if pos_count>neg_count:\n",
    "            for word in keywords['Value']:\n",
    "                if word in article:\n",
    "                    word_dict_pos[word]+=1\n",
    "                    type_dict_pos[keywords[keywords['Value']==word]['Type'].values[0]]+=1\n",
    "        elif pos_count<neg_count:\n",
    "            for word in keywords['Value']:\n",
    "                if word in article:\n",
    "                    word_dict_neg[word]+=1\n",
    "                    type_dict_neg[keywords[keywords['Value']==word]['Type'].values[0]]+=1\n",
    "        else:\n",
    "            for word in keywords['Value']:\n",
    "                if word in article:\n",
    "                    word_dict_gen[word]+=1\n",
    "                    type_dict_gen[keywords[keywords['Value']==word]['Type'].values[0]]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occurences</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>san bernardino</th>\n",
       "      <td>25</td>\n",
       "      <td>-0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tennessee</th>\n",
       "      <td>91</td>\n",
       "      <td>-0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arizona</th>\n",
       "      <td>231</td>\n",
       "      <td>-0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gun control</th>\n",
       "      <td>27</td>\n",
       "      <td>-0.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>checks and balances</th>\n",
       "      <td>21</td>\n",
       "      <td>-0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fascism</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>civil liberties</th>\n",
       "      <td>39</td>\n",
       "      <td>-0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pardon</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veto</th>\n",
       "      <td>17</td>\n",
       "      <td>-0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whip</th>\n",
       "      <td>25</td>\n",
       "      <td>-0.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gag order</th>\n",
       "      <td>22</td>\n",
       "      <td>-0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>louisiana</th>\n",
       "      <td>55</td>\n",
       "      <td>-0.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regime</th>\n",
       "      <td>30</td>\n",
       "      <td>-0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pennsylvania</th>\n",
       "      <td>76</td>\n",
       "      <td>-0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>provo</th>\n",
       "      <td>16</td>\n",
       "      <td>-0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maryland</th>\n",
       "      <td>32</td>\n",
       "      <td>-0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dallas</th>\n",
       "      <td>18</td>\n",
       "      <td>-0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maine</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>massachusetts</th>\n",
       "      <td>90</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indiana</th>\n",
       "      <td>125</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kansas</th>\n",
       "      <td>210</td>\n",
       "      <td>-0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>georgia</th>\n",
       "      <td>176</td>\n",
       "      <td>-0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>independence</th>\n",
       "      <td>34</td>\n",
       "      <td>-0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yield</th>\n",
       "      <td>33</td>\n",
       "      <td>-0.575758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>judicial</th>\n",
       "      <td>54</td>\n",
       "      <td>-0.574074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grain</th>\n",
       "      <td>56</td>\n",
       "      <td>-0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kansas city</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>term limits</th>\n",
       "      <td>21</td>\n",
       "      <td>-0.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boston</th>\n",
       "      <td>43</td>\n",
       "      <td>-0.558140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diversity</th>\n",
       "      <td>18</td>\n",
       "      <td>-0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>florida</th>\n",
       "      <td>1472</td>\n",
       "      <td>-0.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>charter</th>\n",
       "      <td>50</td>\n",
       "      <td>-0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imation</th>\n",
       "      <td>41</td>\n",
       "      <td>-0.268293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nevada</th>\n",
       "      <td>95</td>\n",
       "      <td>-0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unemployment</th>\n",
       "      <td>19</td>\n",
       "      <td>-0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loans</th>\n",
       "      <td>39</td>\n",
       "      <td>-0.256410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bond</th>\n",
       "      <td>43</td>\n",
       "      <td>-0.255814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alabama</th>\n",
       "      <td>80</td>\n",
       "      <td>-0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>directv</th>\n",
       "      <td>28</td>\n",
       "      <td>-0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>los angeles</th>\n",
       "      <td>25</td>\n",
       "      <td>-0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best buy</th>\n",
       "      <td>17</td>\n",
       "      <td>-0.235294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vista</th>\n",
       "      <td>22</td>\n",
       "      <td>-0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orange</th>\n",
       "      <td>42</td>\n",
       "      <td>-0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>statute</th>\n",
       "      <td>19</td>\n",
       "      <td>-0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chattanooga</th>\n",
       "      <td>19</td>\n",
       "      <td>-0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nebraska</th>\n",
       "      <td>30</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seattle</th>\n",
       "      <td>33</td>\n",
       "      <td>-0.181818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new mexico</th>\n",
       "      <td>18</td>\n",
       "      <td>-0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>civil war</th>\n",
       "      <td>26</td>\n",
       "      <td>-0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bundling</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starbucks</th>\n",
       "      <td>20</td>\n",
       "      <td>-0.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dell</th>\n",
       "      <td>33</td>\n",
       "      <td>-0.121212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alaska</th>\n",
       "      <td>34</td>\n",
       "      <td>-0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>socialism</th>\n",
       "      <td>36</td>\n",
       "      <td>-0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dividend</th>\n",
       "      <td>18</td>\n",
       "      <td>-0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converse</th>\n",
       "      <td>21</td>\n",
       "      <td>-0.095238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minnesota</th>\n",
       "      <td>44</td>\n",
       "      <td>-0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>centurylink</th>\n",
       "      <td>26</td>\n",
       "      <td>-0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>montana</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inflation</th>\n",
       "      <td>38</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>274 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Occurences    Rating\n",
       "san bernardino               25 -0.800000\n",
       "tennessee                    91 -0.769231\n",
       "arizona                     231 -0.761905\n",
       "gun control                  27 -0.740741\n",
       "checks and balances          21 -0.714286\n",
       "fascism                      20 -0.700000\n",
       "civil liberties              39 -0.692308\n",
       "pardon                       20 -0.650000\n",
       "veto                         17 -0.647059\n",
       "whip                         25 -0.640000\n",
       "gag order                    22 -0.636364\n",
       "louisiana                    55 -0.636364\n",
       "regime                       30 -0.633333\n",
       "pennsylvania                 76 -0.631579\n",
       "provo                        16 -0.625000\n",
       "maryland                     32 -0.625000\n",
       "dallas                       18 -0.611111\n",
       "maine                        28 -0.607143\n",
       "massachusetts                90 -0.600000\n",
       "indiana                     125 -0.600000\n",
       "kansas                      210 -0.600000\n",
       "georgia                     176 -0.590909\n",
       "independence                 34 -0.588235\n",
       "yield                        33 -0.575758\n",
       "judicial                     54 -0.574074\n",
       "grain                        56 -0.571429\n",
       "kansas city                  28 -0.571429\n",
       "term limits                  21 -0.571429\n",
       "boston                       43 -0.558140\n",
       "diversity                    18 -0.555556\n",
       "...                         ...       ...\n",
       "florida                    1472 -0.281250\n",
       "charter                      50 -0.280000\n",
       "imation                      41 -0.268293\n",
       "nevada                       95 -0.263158\n",
       "unemployment                 19 -0.263158\n",
       "loans                        39 -0.256410\n",
       "bond                         43 -0.255814\n",
       "alabama                      80 -0.250000\n",
       "directv                      28 -0.250000\n",
       "los angeles                  25 -0.240000\n",
       "best buy                     17 -0.235294\n",
       "vista                        22 -0.227273\n",
       "orange                       42 -0.214286\n",
       "statute                      19 -0.210526\n",
       "chattanooga                  19 -0.210526\n",
       "nebraska                     30 -0.200000\n",
       "seattle                      33 -0.181818\n",
       "new mexico                   18 -0.166667\n",
       "civil war                    26 -0.153846\n",
       "bundling                     20 -0.150000\n",
       "starbucks                    20 -0.150000\n",
       "dell                         33 -0.121212\n",
       "alaska                       34 -0.117647\n",
       "socialism                    36 -0.111111\n",
       "dividend                     18 -0.111111\n",
       "converse                     21 -0.095238\n",
       "minnesota                    44 -0.090909\n",
       "centurylink                  26 -0.038462\n",
       "montana                      20  0.000000\n",
       "inflation                    38  0.052632\n",
       "\n",
       "[274 rows x 2 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_results=pd.DataFrame([word_dict_neg,word_dict_pos,word_dict_gen]).T\n",
    "word_results.columns=['Negative','Positive',\"General\"]\n",
    "word_results[\"Occurences\"]=word_results[\"Positive\"]+word_results[\"Negative\"]+word_results[\"General\"]\n",
    "word_results[\"Rating\"]=(word_results[\"Positive\"]-word_results['Negative'])/word_results[\"Occurences\"]\n",
    "word_results[word_results['Occurences']>15].sort_values(\"Rating\")[[\"Occurences\",\"Rating\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
