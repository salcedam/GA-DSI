{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"I used the documentation in addition to the following video for help on this webscraping/API process:\n",
    "    https://www.youtube.com/watch?v=yexxcrPC7U8 \"\"\"\n",
    "\n",
    "import requests\n",
    "import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import praw\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below section is for reddit scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_replies(comment, reply_list):\n",
    "    \"\"\" recursive function that goes through the reply tree of a comment \"\"\"\n",
    "    reply_list.append({\"id\": comment.id, \"comment\": comment.body})\n",
    "    if len(comment._replies) > 0:\n",
    "        for r in comment._replies:\n",
    "            get_replies(r, reply_list)\n",
    "    return reply_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Collection from r/Futurology\n",
      "  Finished 20 Out of 300\n",
      "    Time Elapsed: 8.71059374809 minutes\n",
      "  Finished 40 Out of 300\n",
      "    Time Elapsed: 17.3826767166 minutes\n",
      "  Finished 60 Out of 300\n",
      "    Time Elapsed: 26.0483352661 minutes\n",
      "  Finished 80 Out of 300\n",
      "    Time Elapsed: 34.7198677659 minutes\n",
      "  Finished 100 Out of 300\n",
      "    Time Elapsed: 43.4012860497 minutes\n",
      "  Finished 120 Out of 300\n",
      "    Time Elapsed: 52.1055244684 minutes\n",
      "  Finished 140 Out of 300\n",
      "    Time Elapsed: 60.7756651998 minutes\n",
      "  Finished 160 Out of 300\n",
      "    Time Elapsed: 69.4521248182 minutes\n",
      "  Finished 180 Out of 300\n",
      "    Time Elapsed: 78.1329490145 minutes\n",
      "  Finished 200 Out of 300\n",
      "    Time Elapsed: 86.803851517 minutes\n",
      "  Finished 220 Out of 300\n",
      "    Time Elapsed: 95.5034796675 minutes\n",
      "  Finished 240 Out of 300\n",
      "    Time Elapsed: 104.171271618 minutes\n",
      "  Finished 260 Out of 300\n",
      "    Time Elapsed: 112.703770681 minutes\n",
      "  Finished 280 Out of 300\n",
      "    Time Elapsed: 120.543548282 minutes\n",
      "  Finished 300 Out of 300\n",
      "    Time Elapsed: 129.212287815 minutes\n",
      "Time Spent: 129.242125467 minutes\n",
      "Finished Collection from r/Futurology\n",
      "\n",
      "Starting Collection from r/StockMarket\n",
      "  Finished 20 Out of 300\n",
      "    Time Elapsed: 0.705569636822 minutes\n",
      "  Finished 40 Out of 300\n",
      "    Time Elapsed: 1.37136155367 minutes\n",
      "  Finished 60 Out of 300\n",
      "    Time Elapsed: 2.03889041742 minutes\n",
      "  Finished 80 Out of 300\n",
      "    Time Elapsed: 2.70318055153 minutes\n",
      "  Finished 100 Out of 300\n",
      "    Time Elapsed: 3.40285121997 minutes\n",
      "  Finished 120 Out of 300\n",
      "    Time Elapsed: 4.13699860175 minutes\n",
      "  Finished 140 Out of 300\n",
      "    Time Elapsed: 4.80348876715 minutes\n",
      "  Finished 160 Out of 300\n",
      "    Time Elapsed: 5.47439851761 minutes\n",
      "  Finished 180 Out of 300\n",
      "    Time Elapsed: 6.13732075294 minutes\n",
      "  Finished 200 Out of 300\n",
      "    Time Elapsed: 6.80571986834 minutes\n",
      "  Finished 220 Out of 300\n",
      "    Time Elapsed: 7.50394970179 minutes\n",
      "  Finished 240 Out of 300\n",
      "    Time Elapsed: 8.17685613632 minutes\n",
      "  Finished 260 Out of 300\n",
      "    Time Elapsed: 8.843265903 minutes\n",
      "  Finished 280 Out of 300\n",
      "    Time Elapsed: 9.5064860185 minutes\n",
      "  Finished 300 Out of 300\n",
      "    Time Elapsed: 10.1743810177 minutes\n",
      "Time Spent: 10.175907135 minutes\n",
      "Finished Collection from r/StockMarket\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subreddit_dictionary={}\n",
    "title_list=[]\n",
    "\n",
    "###\n",
    "Number_of_Posts=300\n",
    "Number_of_Comments=12\n",
    "\n",
    "\n",
    "\n",
    "subreddit_list=[\"technology\",\"politics\",\"news\",\"worldnews\",\"inthenews\",\"Futurology\",\"StockMarket\"]\n",
    "\n",
    "for i in range(5,len(subreddit_list)):\n",
    "    count=0\n",
    "    current_time=time.time()\n",
    "    print \"Starting Collection from r/\"+subreddit_list[i]\n",
    "    subreddit_posts={}\n",
    "    json_name='/Users/thomas/GA-DSI/projects/projects-capstone/Work/r_'\\\n",
    "                        +subreddit_list[i]+'.json'\n",
    "    info_for_request='get top '+str(Number_of_Posts)+' posts w/ top '+str(Number_of_Comments)+\\\n",
    "                        ' comments and children comments from r/'+subreddit_list[i]+\\\n",
    "                        \"for Capstone project at General Assembly's DSI program by User: dingdong1111\"\n",
    "    r = praw.Reddit(user_agent=info_for_request)\n",
    "    r.login('dingdong1111', '01001759',disable_warning=True)\n",
    "    posts = r.get_subreddit(subreddit_list[i]).get_top_from_year(limit=Number_of_Posts)\n",
    "    for post in posts:\n",
    "        count+=1\n",
    "        post_comments=[]\n",
    "        \n",
    "        post_id=post.id\n",
    "        post_title=post.title\n",
    "        post_score=post.score\n",
    "        post_url=post.url\n",
    "        post_date=datetime.datetime.fromtimestamp(post.created)\n",
    "#         post_author=post.author\n",
    "        \n",
    "        submission = r.get_submission(submission_id=post_id)\n",
    "        submission.replace_more_comments(limit=Number_of_Comments, threshold=0)\n",
    "        for comment in submission.comments:\n",
    "            \"\"\" loops through all comments to submission \"\"\"\n",
    "            x = []  # array that holds the comment and all replies\n",
    "            # get all replies and append it to our array for writing to file\n",
    "            post_comments.append(get_replies(comment, x))\n",
    "            \n",
    "        if count%20==0:\n",
    "            print \"  Finished\",count,\"Out of\",Number_of_Posts\n",
    "            print \"    Time Elapsed:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "        subreddit_posts[post_id]=[post_title,str(post_date),post_score,post_url,post_comments]\n",
    "    with open(json_name, 'w') as fp:\n",
    "        json.dump(subreddit_posts, fp)\n",
    "    \n",
    "    print \"Time Spent:\",(time.time()-current_time)/60.,\"minutes\"\n",
    "    print \"Finished Collection from r/\"+subreddit_list[i]\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"THIS IS THE CODE FROM THE AFOREMENTIONED VIDEO (SEE IMPORT CELL)\"\"\"\n",
    "\n",
    "\n",
    "import praw\n",
    "\n",
    "#key in the submission ID of the Reddit thread you want to scrape\n",
    "sub_id = \"xxxxxx\"\n",
    "\n",
    "def get_replies(comment, reply_list):\n",
    "    \"\"\" recursive function that goes through the reply tree of a comment \"\"\"\n",
    "    reply_list.append({\"id\": comment.id, \"comment\": comment.body})\n",
    "    if len(comment._replies) > 0:\n",
    "        for r in comment._replies:\n",
    "            get_replies(r, reply_list)\n",
    "    return reply_list\n",
    "\n",
    "\n",
    "#state your purpose for conducting the comment scrape\n",
    "r = praw.Reddit('get comments from xxxxxxxx for xxxxxxx by User: xxxxxxxxx')\n",
    "submission = r.get_submission(submission_id=sub_id)\n",
    "\"\"\" this is a costly operation due to reddit's limits but is necessary\n",
    "    to get all comments and prevent errors in script \"\"\"\n",
    "submission.replace_more_comments(limit=None, threshold=0)\n",
    "\n",
    "# final array for writing to file\n",
    "final = []\n",
    "\n",
    "for comment in submission.comments:\n",
    "    \"\"\" loops through all comments to submission \"\"\"\n",
    "    x = []  # array that holds the comment and all replies\n",
    "    # get all replies and append it to our array for writing to file\n",
    "    final.append(get_replies(comment, x))\n",
    "\n",
    "with open('comments.csv', 'a') as outf:\n",
    "    \"\"\" open file and write to it \"\"\"\n",
    "    # 2 dimension array needs two for loops\n",
    "    for ele in final:\n",
    "        for ele2 in ele:\n",
    "            # write to file in the proper comma delimited format, one per line\n",
    "            outf.write(u\"{0},{1}\\n\".format(ele2['id'], ele2['comment']).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The below section is for federalreserve.gov scraping\n",
    "\n",
    "https://www.federalreserve.gov/newsevents/press/all/2016all.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fed_dict={}\n",
    "\n",
    "sites=[\"https://www.federalreserve.gov/newsevents/press/all/2016all.htm\",\\\n",
    "       \"https://www.federalreserve.gov/newsevents/press/all/2015all.htm\"]\n",
    "r=[]\n",
    "for site in sites:\n",
    "    r.append(requests.get(site))\n",
    "for i in range(len(r)):\n",
    "    soup=BeautifulSoup.BeautifulSoup(r[i].text)\n",
    "    urls=['https://www.federalreserve.gov'+str(x.find('a')['href']) for x in soup.findAll(\"div\",{'class':'indent'})]\n",
    "    titles=[x.find('a').text for x in soup.findAll(\"div\",{'class':'indent'})]\n",
    "    dates=[str('https://www.federalreserve.gov'+str(x.find('a')['href']))[-13:-5] \\\n",
    "                     for x in soup.findAll(\"div\",{'class':'indent'})]\n",
    "    documents=[]\n",
    "    for url in urls:\n",
    "        r2=requests.get(url)\n",
    "        soup=BeautifulSoup.BeautifulSoup(r2.text)\n",
    "        documents.append([x.text for x in soup.findAll('p')[1:]])\n",
    "    for a,b,c,d in zip(dates,titles,urls,documents):\n",
    "        fed_dict[a]=[b,c,d]\n",
    "with open(\"/Users/thomas/GA-DSI/projects/projects-capstone/Work/fed_info.json\", 'w') as fp:\n",
    "    json.dump(fed_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20161110',\n",
       " '20161103',\n",
       " '20161102',\n",
       " '20161101',\n",
       " '20161031',\n",
       " '20161028',\n",
       " '20161027',\n",
       " '20161027',\n",
       " '20161025',\n",
       " '20161025',\n",
       " '20161024',\n",
       " '20161024',\n",
       " '20161024',\n",
       " '20161021',\n",
       " '20161020',\n",
       " '20161020',\n",
       " '20161019',\n",
       " '20161018',\n",
       " '20161014',\n",
       " '20161013',\n",
       " '20161012',\n",
       " '20161006',\n",
       " '20161004',\n",
       " '20161004',\n",
       " '20161004',\n",
       " '20160929',\n",
       " '20160926',\n",
       " '20160923',\n",
       " '20160921',\n",
       " '20160921',\n",
       " '20160913',\n",
       " '20160909',\n",
       " '20160908',\n",
       " '20160908',\n",
       " '20160906',\n",
       " '20160901',\n",
       " '20160829',\n",
       " '20160825',\n",
       " '20160823',\n",
       " '20160823',\n",
       " '20160818',\n",
       " '20160818',\n",
       " '20160817',\n",
       " '20160816',\n",
       " '20160811',\n",
       " '20160811',\n",
       " '20160809',\n",
       " '20160809',\n",
       " '20160808',\n",
       " '20160804',\n",
       " '20160803',\n",
       " '20160802',\n",
       " '20160802',\n",
       " '20160802',\n",
       " '20160802',\n",
       " '20160801',\n",
       " '20160801',\n",
       " '20160729',\n",
       " '20160729',\n",
       " '20160728',\n",
       " '20160728',\n",
       " '20160727',\n",
       " '20160725',\n",
       " '20160722',\n",
       " '20160722',\n",
       " '20160721',\n",
       " '20160719',\n",
       " '20160715',\n",
       " '20160714',\n",
       " '20160712',\n",
       " '20160712',\n",
       " '20160707',\n",
       " '20160706',\n",
       " '20160706',\n",
       " '20160630',\n",
       " '20160629',\n",
       " '20160628',\n",
       " '20160628',\n",
       " '20160628',\n",
       " '20160628',\n",
       " '20160624',\n",
       " '20160624',\n",
       " '20160623',\n",
       " '20160617',\n",
       " '20160617',\n",
       " '20160617',\n",
       " '20160617',\n",
       " '20160616',\n",
       " '20160615',\n",
       " '20160615',\n",
       " '20160614',\n",
       " '20160610',\n",
       " '20160610',\n",
       " '20160609',\n",
       " '20160608',\n",
       " '20160603',\n",
       " '20160602',\n",
       " '20160602',\n",
       " '20160531',\n",
       " '20160525',\n",
       " '20160524',\n",
       " '20160524',\n",
       " '20160519',\n",
       " '20160518',\n",
       " '20160517',\n",
       " '20160516',\n",
       " '20160512',\n",
       " '20160511',\n",
       " '20160509',\n",
       " '20160505',\n",
       " '20160505',\n",
       " '20160503',\n",
       " '20160503',\n",
       " '20160502',\n",
       " '20160502',\n",
       " '20160503',\n",
       " '20160427',\n",
       " '20160426',\n",
       " '20160422',\n",
       " '20160421',\n",
       " '20160420',\n",
       " '20160419',\n",
       " '20160419',\n",
       " '20160414',\n",
       " '20160413',\n",
       " '20160412',\n",
       " '20160412',\n",
       " '20160407',\n",
       " '20160406',\n",
       " '20160401',\n",
       " '20160330',\n",
       " '20160329',\n",
       " '20160324',\n",
       " '20160324',\n",
       " '20160323',\n",
       " '20160322',\n",
       " '20160321',\n",
       " '20160321',\n",
       " '20160317',\n",
       " '20160316',\n",
       " '20160316',\n",
       " '20160314',\n",
       " '20160308',\n",
       " '20160304',\n",
       " '20160304',\n",
       " '20160303',\n",
       " '20160301',\n",
       " '20160223',\n",
       " '20160219',\n",
       " '20160219',\n",
       " '20160218',\n",
       " '20160218',\n",
       " '20160218',\n",
       " '20160217',\n",
       " '20160217',\n",
       " '20160212',\n",
       " '20160211',\n",
       " '20160205',\n",
       " '20160203',\n",
       " '20160202',\n",
       " '20160202',\n",
       " '20160129',\n",
       " '20160129',\n",
       " '20160128',\n",
       " '20160127',\n",
       " '20160127',\n",
       " '20160121',\n",
       " '20160119',\n",
       " '20160119',\n",
       " '20160112',\n",
       " '20160111',\n",
       " '20160107',\n",
       " '20160106',\n",
       " '20160106']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
